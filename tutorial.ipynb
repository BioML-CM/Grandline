{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grandline tutorial\n",
    "\n",
    "1. Load synthetic linear Data (200 samples, 5000 genes/nodes, 20 clusters)\n",
    "    - `cv_data_dict[i]['X_train']` (160 sample x 5000 node)\n",
    "    - `cv_data_dict[i]['X_test']` (40 sample x 5000 node)\n",
    "    - `cv_data_dict[i]['y_train']` (160 sample x 1)\n",
    "    - `cv_data_dict[i]['y_test']` (40 sample x 1)\n",
    "    - `i=0,1,...,9` (10 shuffles)\n",
    "2. Create adjacency matrix A \n",
    "3. Set GCN hyperparameters :\n",
    "`epoch, learning rate, regularization, batch_size, number of graph convolutional filters(Fs), polynomial orders(Ks), pooling sizes(Ps), fully connected layers(Ms)`\n",
    "\n",
    "4. Train model\n",
    "5. ทดสอบ prediction\n",
    "6. GradCAM\n",
    "    - ค่าความสำคัญของแต่ละ sample\n",
    "    - ค่าความสำคัญของแต่ละ class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:35:39.701520Z",
     "start_time": "2020-11-08T10:35:39.698940Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx \n",
    "import scipy\n",
    "import pickle, os\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from lib import  graph, coarsening, utils, grandline #models,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:35:39.726063Z",
     "start_time": "2020-11-08T10:35:39.723783Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:35:40.124542Z",
     "start_time": "2020-11-08T10:35:39.727249Z"
    }
   },
   "outputs": [],
   "source": [
    "disease = 'synthetic'\n",
    "disease_type = 'X_linear'\n",
    "input_name = 'RandomPartition_5000_20'\n",
    "input_prefix = 'data/{}_{}'.format(disease_type, input_name)\n",
    "\n",
    "cv_data_dict = pickle.load(open(\"{}_cv.pickle\".format(input_prefix), \"rb\"))\n",
    "n_shuffle = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of genes 5000\n",
      "List of genes Index(['N00000', 'N00001', 'N00002', 'N00003', 'N00004', 'N00005', 'N00006',\n",
      "       'N00007', 'N00008', 'N00009',\n",
      "       ...\n",
      "       'N04990', 'N04991', 'N04992', 'N04993', 'N04994', 'N04995', 'N04996',\n",
      "       'N04997', 'N04998', 'N04999'],\n",
      "      dtype='object', length=5000)\n"
     ]
    }
   ],
   "source": [
    "gene_list = cv_data_dict[0]['X_train'].columns\n",
    "d = len(gene_list)\n",
    "\n",
    "print (\"Number of genes\", d)\n",
    "print (\"List of genes\", gene_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes 2\n"
     ]
    }
   ],
   "source": [
    "# จำนวน class\n",
    "temp_df = cv_data_dict[0]['y_train']\n",
    "C = temp_df.groupby(temp_df.columns[0]).size().shape[0]\n",
    "print (\"Number of classes\", C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change DataFrame to numpy array and reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose shuffle\n",
    "current_shuffle = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:35:40.242296Z",
     "start_time": "2020-11-08T10:35:40.152222Z"
    }
   },
   "outputs": [],
   "source": [
    "for name in ['X_train', 'X_test']:    \n",
    "    cv_data_dict[current_shuffle][name]= cv_data_dict[current_shuffle][name].values.astype(np.float32)\n",
    "\n",
    "for name in ['y_train', 'y_test']:    \n",
    "    cv_data_dict[current_shuffle][name] = cv_data_dict[current_shuffle][name].values.astype(np.uint8)\n",
    "        \n",
    "\n",
    "cv_data_dict[current_shuffle]['y_test'] = cv_data_dict[current_shuffle]['y_test'].reshape((cv_data_dict[current_shuffle]['y_test'].shape[0],))\n",
    "cv_data_dict[current_shuffle]['y_train'] = cv_data_dict[current_shuffle]['y_train'].reshape((cv_data_dict[current_shuffle]['y_train'].shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = cv_data_dict[current_shuffle]['X_train']\n",
    "y_train = cv_data_dict[current_shuffle]['y_train']\n",
    "X_test = cv_data_dict[current_shuffle]['X_test']\n",
    "y_test = cv_data_dict[current_shuffle]['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create adjacency matrix A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:35:40.568848Z",
     "start_time": "2020-11-08T10:35:40.249247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created A 5000x5000\n"
     ]
    }
   ],
   "source": [
    "A = utils.prepare_adjacency('data/A_{}.csv'.format(input_name), gene_list)\n",
    "print (\"Created A {}x{}\".format(A.shape[0], A.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplacian function\n",
    "\n",
    "L is calculated from A without diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:35:40.583874Z",
     "start_time": "2020-11-08T10:35:40.573371Z"
    }
   },
   "outputs": [],
   "source": [
    "#Move!!\n",
    "# def calculate_laplacian(A, levels, seed=1):\n",
    "#     np.random.seed(seed)\n",
    "#     graphs, perms = coarsening.coarsen(A, levels=levels, self_connections=False)\n",
    "#     L = [graph.laplacian(A, normalized=True) for A in graphs]\n",
    "    \n",
    "#     return L, graphs, perms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:35:40.605918Z",
     "start_time": "2020-11-08T10:35:40.584843Z"
    }
   },
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['num_epochs']     = 15\n",
    "params['learning_rate']  = 1e-3\n",
    "params['filter_name']    = 'chebyshev'\n",
    "\n",
    "seed = 8\n",
    "\n",
    "params['Fs']              = [20, 20]  # Number of graph convolutional filters. \n",
    "params['Ks']              = [10, 10]  # Polynomial orders.\n",
    "params['Ps']              = [2, 2]  # Pooling sizes. \n",
    "params['Ms']              = [C]  # Output dimensionality of fully connected layers.\n",
    "\n",
    "params['regularization'] = 1e-5\n",
    "params['batch_size'] = X_train.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate normalized L for each level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:36:08.993960Z",
     "start_time": "2020-11-08T10:35:40.616668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coarsening level: 2\n",
      "Layer 0: M_0 = |V| = 5032 nodes (32 added),|E| = 307436 edges\n",
      "Layer 1: M_1 = |V| = 2516 nodes (5 added),|E| = 218625 edges\n",
      "Layer 2: M_2 = |V| = 1258 nodes (0 added),|E| = 123181 edges\n"
     ]
    }
   ],
   "source": [
    "n_level_coarsen = int(np.log2(params['Ps']).sum())\n",
    "print (\"Coarsening level:\", n_level_coarsen)\n",
    "Ls, graphs, perms = graph.calculate_laplacian(A, levels=n_level_coarsen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrange features/genes according to permutation (from coarsening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:36:09.501784Z",
     "start_time": "2020-11-08T10:36:09.036076Z"
    }
   },
   "outputs": [],
   "source": [
    "if perms is not None:\n",
    "    X_train = coarsening.perm_data(X_train, perms[0])\n",
    "    X_test = coarsening.perm_data(X_test, perms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:36:09.505495Z",
     "start_time": "2020-11-08T10:36:09.502670Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical \n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:36:09.585956Z",
     "start_time": "2020-11-08T10:36:09.561056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160, 5032, 1), (40, 5032, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.expand_dims(X_train, 2)\n",
    "X_test = np.expand_dims(X_test, 2)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:36:09.608100Z",
     "start_time": "2020-11-08T10:36:09.586941Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:36:09.785673Z",
     "start_time": "2020-11-08T10:36:09.609048Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model, model_logit = grandline.build_gcn_model(graphs, Ls, **params)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=params['learning_rate'], name='Adam'), \n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \n",
    "              metrics=['accuracy'])\n",
    "model.build(input_shape=X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call back functions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:36:09.818041Z",
     "start_time": "2020-11-08T10:36:09.809664Z"
    }
   },
   "outputs": [],
   "source": [
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                                      min_delta=0.001, \n",
    "                                                      patience=15,\n",
    "                                                      verbose=1,\n",
    "                                                      mode='max',\n",
    "                                                      baseline=None, \n",
    "                                                      restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:36:09.829815Z",
     "start_time": "2020-11-08T10:36:09.818988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0, 1: 1.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "y_train_class_name = np.argmax(y_train, axis=1)\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  np.unique(y_train_class_name),\n",
    "                                                  y_train_class_name)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:36:16.707238Z",
     "start_time": "2020-11-08T10:36:09.861317Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 40 samples\n",
      "Epoch 1/15\n",
      "160/160 [==============================] - 17s 106ms/sample - loss: 0.7290 - accuracy: 0.4938 - val_loss: 0.5427 - val_accuracy: 0.5250\n",
      "Epoch 2/15\n",
      "160/160 [==============================] - 13s 79ms/sample - loss: 0.5442 - accuracy: 0.5188 - val_loss: 0.1159 - val_accuracy: 1.0000\n",
      "Epoch 3/15\n",
      "160/160 [==============================] - 12s 73ms/sample - loss: 0.1315 - accuracy: 1.0000 - val_loss: 0.1064 - val_accuracy: 1.0000\n",
      "Epoch 4/15\n",
      "160/160 [==============================] - 11s 71ms/sample - loss: 0.1453 - accuracy: 1.0000 - val_loss: 0.0693 - val_accuracy: 1.0000\n",
      "Epoch 5/15\n",
      "160/160 [==============================] - 11s 71ms/sample - loss: 0.0990 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 1.0000\n",
      "Epoch 6/15\n",
      "160/160 [==============================] - 11s 71ms/sample - loss: 0.0336 - accuracy: 1.0000 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
      "Epoch 7/15\n",
      "160/160 [==============================] - 11s 71ms/sample - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 8/15\n",
      "160/160 [==============================] - 12s 74ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 9/15\n",
      "160/160 [==============================] - 12s 73ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "160/160 [==============================] - 11s 71ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "160/160 [==============================] - 11s 72ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "160/160 [==============================] - 11s 69ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "160/160 [==============================] - 11s 72ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "160/160 [==============================] - 12s 72ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "160/160 [==============================] - 11s 71ms/sample - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=X_train,\n",
    "                    y=y_train,\n",
    "                    epochs=params['num_epochs'],\n",
    "                    validation_data=[X_test, y_test],\n",
    "                    batch_size=params['batch_size'],\n",
    "                    class_weight=class_weights,\n",
    "                    callbacks=[earlystop_callback], # checkpoint_callback\n",
    "                    verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:36:16.720429Z",
     "start_time": "2020-11-08T10:36:16.710157Z"
    }
   },
   "outputs": [],
   "source": [
    "logit_model = tf.keras.Model(inputs=model_logit.inputs, outputs=model_logit.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = model.predict(x=X_test)\n",
    "np.argmax(predict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:36:37.868233Z",
     "start_time": "2020-11-08T10:36:17.231348Z"
    }
   },
   "outputs": [],
   "source": [
    "num_train = X_train.shape[0]\n",
    "\n",
    "node_label = []\n",
    "node_importance = []\n",
    "\n",
    "for selected_sample_id in range(num_train):\n",
    "    X_input = np.expand_dims(X_train[selected_sample_id], 0).astype('float32')\n",
    "    node_label += [np.argmax(y_train[selected_sample_id])]\n",
    "    node_importance += [grandline.cal_gradcam(selected_sample_id, X_input, logit_model)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ค่า important ของแต่ละ sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hatairat/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#สำหรับ node in last conv\n",
    "importance_allnode_df = pd.DataFrame(np.array([node_label, node_importance]).T, columns=['label', 'important'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node</th>\n",
       "      <th>train_0</th>\n",
       "      <th>train_1</th>\n",
       "      <th>train_2</th>\n",
       "      <th>train_3</th>\n",
       "      <th>train_4</th>\n",
       "      <th>train_5</th>\n",
       "      <th>train_6</th>\n",
       "      <th>train_7</th>\n",
       "      <th>train_8</th>\n",
       "      <th>...</th>\n",
       "      <th>train_150</th>\n",
       "      <th>train_151</th>\n",
       "      <th>train_152</th>\n",
       "      <th>train_153</th>\n",
       "      <th>train_154</th>\n",
       "      <th>train_155</th>\n",
       "      <th>train_156</th>\n",
       "      <th>train_157</th>\n",
       "      <th>train_158</th>\n",
       "      <th>train_159</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>N00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.005491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.001606</td>\n",
       "      <td>0.001673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>N00001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001194</td>\n",
       "      <td>0.002905</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>0.002255</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.001985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004047</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>0.004383</td>\n",
       "      <td>0.001917</td>\n",
       "      <td>0.004368</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>0.003362</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.004877</td>\n",
       "      <td>0.004391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>N00002</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005489</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>0.001718</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.003082</td>\n",
       "      <td>0.003859</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.002315</td>\n",
       "      <td>0.001346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>N00003</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.005526</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.006016</td>\n",
       "      <td>0.004776</td>\n",
       "      <td>0.005589</td>\n",
       "      <td>0.006640</td>\n",
       "      <td>0.005094</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.000145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>N00004</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.005242</td>\n",
       "      <td>0.005293</td>\n",
       "      <td>0.004814</td>\n",
       "      <td>0.005816</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.003648</td>\n",
       "      <td>0.003380</td>\n",
       "      <td>0.006086</td>\n",
       "      <td>0.006143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>0.000452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 161 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        node   train_0   train_1   train_2   train_3   train_4   train_5  \\\n",
       "Id                                                                         \n",
       "N00000   0.0  0.000213  0.003232  0.000543  0.000522  0.002714  0.002878   \n",
       "N00001   1.0  0.001194  0.002905  0.001239  0.002255  0.000218  0.001857   \n",
       "N00002   2.0  0.005489  0.002887  0.001718  0.002107  0.003082  0.003859   \n",
       "N00003   3.0  0.001857  0.005526  0.002899  0.006016  0.004776  0.005589   \n",
       "N00004   4.0  0.005242  0.005293  0.004814  0.005816  0.006061  0.003648   \n",
       "\n",
       "         train_6   train_7   train_8  ...  train_150  train_151  train_152  \\\n",
       "Id                                    ...                                    \n",
       "N00000  0.004862  0.004200  0.005491  ...   0.000943   0.002220   0.001092   \n",
       "N00001  0.002158  0.001485  0.001985  ...   0.004047   0.002359   0.004383   \n",
       "N00002  0.006862  0.000435  0.001886  ...   0.000447   0.000724   0.001299   \n",
       "N00003  0.006640  0.005094  0.005745  ...   0.000585   0.001389   0.000712   \n",
       "N00004  0.003380  0.006086  0.006143  ...   0.001608   0.000435   0.001387   \n",
       "\n",
       "        train_153  train_154  train_155  train_156  train_157  train_158  \\\n",
       "Id                                                                         \n",
       "N00000   0.001653   0.001697   0.000617   0.000501   0.000055   0.001606   \n",
       "N00001   0.001917   0.004368   0.001851   0.003362   0.003968   0.004877   \n",
       "N00002   0.001686   0.001146   0.001637   0.003153   0.001022   0.002315   \n",
       "N00003   0.001740   0.000578   0.001012   0.000027   0.000147   0.001668   \n",
       "N00004   0.000877   0.000852   0.002105   0.001764   0.000553   0.001297   \n",
       "\n",
       "        train_159  \n",
       "Id                 \n",
       "N00000   0.001673  \n",
       "N00001   0.004391  \n",
       "N00002   0.001346  \n",
       "N00003   0.000145  \n",
       "N00004   0.000452  \n",
       "\n",
       "[5 rows x 161 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#สำหรับ node in graph\n",
    "i=0\n",
    "ipt = np.abs(importance_allnode_df['important'][0])\n",
    "label = importance_allnode_df['label'][0]\n",
    "ipt_df = utils.get_node_importance_df(perms, ipt, d)[['node','important']]\n",
    "ipt_df = ipt_df.rename(columns = {'important':'train_{}'.format(0)})\n",
    "\n",
    "for i in range(1,num_train):\n",
    "    ipt = np.abs(importance_allnode_df['important'][i])\n",
    "    label = importance_allnode_df['label'][i]\n",
    "    ipt_df2 = utils.get_node_importance_df(perms, ipt, d)[['important']]\n",
    "    ipt_df2 = ipt_df2.rename(columns = {'important':'train_{}'.format(i)})\n",
    "    ipt_df= pd.concat([ipt_df,ipt_df2],axis=1)\n",
    "\n",
    "ipt_df.loc[:, 'Id'] = gene_list\n",
    "ipt_df = ipt_df.set_index('Id')\n",
    "ipt_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ผลรวมค่า important ของทุก sample แยกตาม class 0/1\n",
    "(sum / sum of absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:36:37.946161Z",
     "start_time": "2020-11-08T10:36:37.919033Z"
    }
   },
   "outputs": [],
   "source": [
    "#สำหรับ node in last conv \n",
    "important_label0 = importance_allnode_df[importance_allnode_df['label']==0]['important'].sum()\n",
    "important_label0_abs = np.abs(importance_allnode_df[importance_allnode_df['label']==0]['important']).sum()\n",
    "\n",
    "important_label1 = importance_allnode_df[importance_allnode_df['label']==1]['important'].sum()\n",
    "important_label1_abs = np.abs(importance_allnode_df[importance_allnode_df['label']==1]['important']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T10:36:52.767959Z",
     "start_time": "2020-11-08T10:36:52.716853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_0</th>\n",
       "      <th>node</th>\n",
       "      <th>cluster</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_0_abs</th>\n",
       "      <th>label_1_abs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>N00000</td>\n",
       "      <td>0.202143</td>\n",
       "      <td>0</td>\n",
       "      <td>2240</td>\n",
       "      <td>-0.084943</td>\n",
       "      <td>0.227807</td>\n",
       "      <td>0.110702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>N00001</td>\n",
       "      <td>0.154467</td>\n",
       "      <td>1</td>\n",
       "      <td>1004</td>\n",
       "      <td>-0.250200</td>\n",
       "      <td>0.154467</td>\n",
       "      <td>0.250200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>N00002</td>\n",
       "      <td>0.298304</td>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.118025</td>\n",
       "      <td>0.298304</td>\n",
       "      <td>0.124068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>N00003</td>\n",
       "      <td>0.429635</td>\n",
       "      <td>3</td>\n",
       "      <td>286</td>\n",
       "      <td>-0.096839</td>\n",
       "      <td>0.429635</td>\n",
       "      <td>0.114606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>N00004</td>\n",
       "      <td>0.411817</td>\n",
       "      <td>4</td>\n",
       "      <td>1584</td>\n",
       "      <td>-0.080547</td>\n",
       "      <td>0.411817</td>\n",
       "      <td>0.091072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label_0  node  cluster   label_1  label_0_abs  label_1_abs\n",
       "Id                                                                 \n",
       "N00000  0.202143     0     2240 -0.084943     0.227807     0.110702\n",
       "N00001  0.154467     1     1004 -0.250200     0.154467     0.250200\n",
       "N00002  0.298304     2     2017  0.118025     0.298304     0.124068\n",
       "N00003  0.429635     3      286 -0.096839     0.429635     0.114606\n",
       "N00004  0.411817     4     1584 -0.080547     0.411817     0.091072"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#สำหรับ node in graph\n",
    "imp_label0_df = utils.get_node_importance_df(perms, important_label0, d)\n",
    "imp_label0_abs_df = utils.get_node_importance_df(perms, important_label0_abs, d)\n",
    "\n",
    "imp_label1_df = utils.get_node_importance_df(perms, important_label1, d)\n",
    "imp_label1_abs_df = utils.get_node_importance_df(perms, important_label1_abs, d)\n",
    "\n",
    "\n",
    "node_df = imp_label0_df\n",
    "node_df.loc[:, 'node'] = node_df['node'].astype(int)\n",
    "node_df.loc[:, 'cluster'] = node_df['cluster'].astype(int)\n",
    "\n",
    "node_df.loc[:, 'Id'] = gene_list\n",
    "node_df = node_df.set_index('Id')\n",
    "node_df = node_df.rename(columns={'important':'label_0'})\n",
    "\n",
    "node_df.loc[:, 'label_1'] = imp_label1_df['important'].values\n",
    "node_df.loc[:, 'label_0_abs'] = imp_label0_abs_df['important'].values\n",
    "node_df.loc[:, 'label_1_abs'] = imp_label1_abs_df['important'].values\n",
    "\n",
    "node_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "364px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
